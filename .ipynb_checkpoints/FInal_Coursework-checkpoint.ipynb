{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this report, I am going to build a neural network to classify clothing images into 10 different fashion categories. Each of the image is associated with only one category. For this purpose, I am using Fashion MNIST dataset from keras dataset. It contains 60,000 images for training and 10,000 images for testing. Each image is 28x28 grayscale, associated with a label from 10 fashion categories. The class labels are:\n",
    "\n",
    "| **Label**       | 0           | 1       | 2        | 3     | 4    | 5      | 6     | 7       | 8    | 9          |\n",
    "|-------------|-------------|---------|----------|-------|------|--------|-------|---------|------|------------|\n",
    "| **Description** | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal | Shirt | Sneaker | Bag  | Ankle boot |\n",
    "\n",
    "Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel. This pixel-value is an integer between 0 and 255. In dataset each row is a separate image. It has 785 columns. The first column consists of the class labels and represents the article of clothing. The rest of the columns contain the pixel values of the associated image. So, 784 columns with pixel values of images. I will build a model that can place an image into one of the 10 class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method\n",
    "The workflow will be as follows: First, training data (train_images and train_labels) will be passed into the neural network. The network will then learn to associate images and labels. Finally, the network will be tested by predicting for test_images, and verified whether these predictions match the labels from test_labels.\n",
    "\n",
    "## Load Dataset\n",
    "At first, it is important to import all the required library and load the dataset from keras. The data are already divided into two sets: one for taining and another for testing purpose. The variables *x_train* and *x_test* are list of images which is represented by values from 0 to 255. *y_train* and *y_test* takes value from 0 to 9 representing the label of fashion categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eruba\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "((x_train,y_train),(x_test,y_test))=fashion_mnist.load_data()\n",
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "It is not possible to feed lists of integers into a neural network. Moreover, different layers are appropriate for different tensor formates and different types of data processing. I am going to use *densely connected* layers which is suitable for 2D tensors. In the dataset, actual images are 8 bit integers in 3D (6000,28,28) tensor formate. It needs to be reshaped and scaled within the interval of [0,1] so that the network can accept it as input. I convert these 2D matrix of 28x28 pixel images into 784 pixel of 1D float32 type matrix. To represent the label, I am using one-hot-encoding which turn the list of numeric or categorical values into the vectors of 0s and 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000, 10) (60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape((60000, 28 * 28))\n",
    "x_train = x_train.astype('float32') / 255\n",
    "\n",
    "x_test = x_test.reshape((10000, 28 * 28))\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(x_test.shape,y_test.shape,x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "- To estimate how well the model has been trained\n",
    "- To estimate the hyperparameter values (like learning rate, number of layer, number of hidden unit,loss function)\n",
    "- I have used 25% of training data for validation set in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 784) (45000, 784) (15000, 10) (45000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:15000]\n",
    "x_partial_train = x_train[15000:]\n",
    "\n",
    "y_val = y_train[:15000]\n",
    "y_partial_train = y_train[15000:]\n",
    "print(x_val.shape,x_partial_train.shape,y_val.shape,y_partial_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "Neural network is a combination of following objects:\n",
    "- Layers, which are added up to the network\n",
    "- Input data and associated output data\n",
    "- The loss function, which defines the feedback signal for learning\n",
    "- The optimizer, which regulates how learning proceeds\n",
    "\n",
    "### Layers: \n",
    "- Layers are the main building block of a neural network. I have tried different combinations of networks and finally come up with a 64 units *Densely connected* model with 2 layers. \n",
    "- Each layer has *relu* activation function except last one \n",
    "- The end layer is size of 10. This means for each input sample, the network will output a 10-dimensional vector that will encode a different output class.\n",
    "- The last layer uses a softmax activation. It means the network will output a probability distribution over the 10 different output classes—for every input sample. The 10 class scores will sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "def create_model(layer_list,mid_activation,last_activation,option):\n",
    "    model = models.Sequential()\n",
    "    if(option==1):\n",
    "    ##Add weight regularization\n",
    "        model.add(layers.Dense(layer_list[0],kernel_regularizer=regularizers.l2(0.0001),\n",
    "                           activation = mid_activation, input_shape = (28*28,)))\n",
    "        for s in layer_list[1:]:\n",
    "            model.add(layers.Dense(s, kernel_regularizer=regularizers.l2(0.0001),\n",
    "                                   activation = mid_activation))\n",
    "        model.add(layers.Dense(10, activation = last_activation))\n",
    "        return model\n",
    "    #Add drop out\n",
    "    elif(option==2):\n",
    "        model.add(layers.Dense(layer_list[0],kernel_regularizer=regularizers.l2(0.0001),\n",
    "                           activation = mid_activation, input_shape = (28*28,)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        for s in layer_list[1:]:\n",
    "            model.add(layers.Dense(s, kernel_regularizer=regularizers.l2(0.0001),\n",
    "                                   activation = mid_activation))\n",
    "            model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(10, activation = last_activation))\n",
    "        return model\n",
    "    else:\n",
    "        model.add(layers.Dense(layer_list[0],\n",
    "                           activation = mid_activation, input_shape = (28*28,)))\n",
    "        for s in layer_list[1:]:\n",
    "            model.add(layers.Dense(s, activation = mid_activation))\n",
    "        model.add(layers.Dense(10, activation = last_activation))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss function: \n",
    "- This function decides quantity that will be minimized during training. As it is a multiclass classification problem, I am using categorical crossentropy as loss function. It measures the distance between two probability distributions. One for the output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, the network will predict output as close as possible to the true labels.\n",
    "\n",
    "### Optimizer:\n",
    "- Determines how the network will be updated based on the loss function.\n",
    "- I am using rmsprop optimizer for this model\n",
    "\n",
    "### Others:\n",
    "- Batch size: As bigger batch size makes learning slow and smaller one causes less repeating pattern, so I took batch size 300.\n",
    "- Number of Epochs: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,loss_function,learning_rate,option):\n",
    "       \n",
    "    if(option == 1):\n",
    "        model.compile(optimizer = optimizers.RMSprop(lr = learning_rate),\n",
    "                  loss = loss_function,\n",
    "                  metrics = ['acc'])\n",
    "    else:\n",
    "        decay_v = learning_rate/50 #epoc\n",
    "        sgd = optimizers.SGD(lr=learning_rate, decay=decay_v, momentum=0.8, nesterov=False)\n",
    "        model.compile(optimizer = sgd,\n",
    "                  loss = loss_function,\n",
    "                  metrics = ['acc'])\n",
    "\n",
    "        \n",
    "\n",
    "    history = model.fit(x_partial_train, \n",
    "                    y_partial_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))\n",
    "    history_dict = history.history\n",
    "    return history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_result(history_list):\n",
    "    for x in range(len(history_list)):\n",
    "        val_loss = history_list[x]['val_loss'] \n",
    "        val_acc = history_list[x]['val_acc'] \n",
    "        print('Validation accuracy: ',max(val_acc), 'Minimum loss position:',np.argmin(val_loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def create_graph(history_list):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #activation_list=['relu','tanh']\n",
    "    #activation_list=['softmax','sigmoid']\n",
    "    lr_list=[0.0001,0.001,0.01]\n",
    "    #node=[16]\n",
    "    for x in range(len(history_list)):\n",
    "        print(max(history_list[x]['val_acc']),' ',str(np.argmin(history_list[x]['val_loss'])))\n",
    "        epochs = range(1, len(history_list[0]['loss']) + 1)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(epochs,history_list[x]['acc'],label = 'T_acc for lr='+str(lr_list[x]))\n",
    "        plt.plot(epochs,history_list[x]['val_acc'],label = 'V_acc for lr='+str(lr_list[x]))\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(epochs,history_list[x]['loss'],label = 'T_loss for lr='+str(lr_list[x]))\n",
    "        plt.plot(epochs,history_list[x]['val_loss'],label = 'V_loss for lr='+str(lr_list[x]))\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "#### Layer Size and Hidden Unit\n",
    "- Train the network with 16,32,64,128,512 unit and 2,3,4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "for nodes in [16,32,64,128,512]: \n",
    "    for layer in range(1,4):\n",
    "        model = create_model([nodes]*layer,'relu','softmax',0)\n",
    "        all_history = train_model(model,'categorical_crossentropy',0.001,1)\n",
    "        history_list.append(all_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Activation Function\n",
    " - Train the data over 2 layers network with 64 unit\n",
    " - For each hidden layer, consider *relu* and *tanh* activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 1s 16us/step - loss: 0.8442 - acc: 0.7262 - val_loss: 0.6049 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.5521 - acc: 0.8099 - val_loss: 0.5011 - val_acc: 0.8285\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.4882 - acc: 0.8311 - val_loss: 0.4594 - val_acc: 0.8411\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.4535 - acc: 0.8408 - val_loss: 0.4395 - val_acc: 0.8441\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.4282 - acc: 0.8508 - val_loss: 0.4371 - val_acc: 0.8421\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.4132 - acc: 0.8552 - val_loss: 0.4233 - val_acc: 0.8479\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3996 - acc: 0.8589 - val_loss: 0.4067 - val_acc: 0.8536\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3848 - acc: 0.8659 - val_loss: 0.3947 - val_acc: 0.8583\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3757 - acc: 0.8672 - val_loss: 0.3859 - val_acc: 0.8652\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3653 - acc: 0.8698 - val_loss: 0.3756 - val_acc: 0.8642\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3582 - acc: 0.8732 - val_loss: 0.3920 - val_acc: 0.8600\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.3483 - acc: 0.8755 - val_loss: 0.3882 - val_acc: 0.8610\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.3418 - acc: 0.8788 - val_loss: 0.3881 - val_acc: 0.8576\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3374 - acc: 0.8782 - val_loss: 0.3618 - val_acc: 0.8671\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3299 - acc: 0.8809 - val_loss: 0.3640 - val_acc: 0.8685\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.3254 - acc: 0.8835 - val_loss: 0.3466 - val_acc: 0.8725\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3201 - acc: 0.8853 - val_loss: 0.3688 - val_acc: 0.8647\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3154 - acc: 0.8865 - val_loss: 0.3526 - val_acc: 0.8748\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3097 - acc: 0.8880 - val_loss: 0.3472 - val_acc: 0.8732\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3077 - acc: 0.8883 - val_loss: 0.3558 - val_acc: 0.8736\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3022 - acc: 0.8913 - val_loss: 0.3576 - val_acc: 0.8675\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2983 - acc: 0.8923 - val_loss: 0.3519 - val_acc: 0.8715\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 1s 14us/step - loss: 0.2947 - acc: 0.8927 - val_loss: 0.3575 - val_acc: 0.8711\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 1s 14us/step - loss: 0.2893 - acc: 0.8952 - val_loss: 0.3407 - val_acc: 0.8726\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 1s 14us/step - loss: 0.2871 - acc: 0.8951 - val_loss: 0.3544 - val_acc: 0.8706\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 1s 15us/step - loss: 0.2822 - acc: 0.8977 - val_loss: 0.3590 - val_acc: 0.8727\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 1s 14us/step - loss: 0.2809 - acc: 0.8974 - val_loss: 0.3288 - val_acc: 0.8796\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2784 - acc: 0.8988 - val_loss: 0.3402 - val_acc: 0.8771\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2745 - acc: 0.9002 - val_loss: 0.3299 - val_acc: 0.8816\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 1s 18us/step - loss: 0.2697 - acc: 0.9028 - val_loss: 0.3560 - val_acc: 0.8649\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.2677 - acc: 0.9033 - val_loss: 0.3407 - val_acc: 0.8781\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2647 - acc: 0.9034 - val_loss: 0.3259 - val_acc: 0.8827\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.2630 - acc: 0.9029 - val_loss: 0.3385 - val_acc: 0.8745\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2597 - acc: 0.9053 - val_loss: 0.3546 - val_acc: 0.8709\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2572 - acc: 0.9052 - val_loss: 0.3385 - val_acc: 0.8793\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2546 - acc: 0.9066 - val_loss: 0.3235 - val_acc: 0.8842\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2519 - acc: 0.9079 - val_loss: 0.3383 - val_acc: 0.8817\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2519 - acc: 0.9074 - val_loss: 0.3188 - val_acc: 0.8861\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2472 - acc: 0.9106 - val_loss: 0.3290 - val_acc: 0.8813\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.2452 - acc: 0.9108 - val_loss: 0.3209 - val_acc: 0.8841\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2422 - acc: 0.9117 - val_loss: 0.3265 - val_acc: 0.8797\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.2400 - acc: 0.9138 - val_loss: 0.3326 - val_acc: 0.8797\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2394 - acc: 0.9124 - val_loss: 0.3224 - val_acc: 0.8816\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.2367 - acc: 0.9133 - val_loss: 0.3182 - val_acc: 0.8864\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2362 - acc: 0.9139 - val_loss: 0.3129 - val_acc: 0.8845\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2324 - acc: 0.9155 - val_loss: 0.3432 - val_acc: 0.8789\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2298 - acc: 0.9153 - val_loss: 0.3260 - val_acc: 0.8819\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2271 - acc: 0.9170 - val_loss: 0.3182 - val_acc: 0.8819\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2274 - acc: 0.9176 - val_loss: 0.3263 - val_acc: 0.8799\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 1s 13us/step - loss: 0.2238 - acc: 0.9181 - val_loss: 0.3282 - val_acc: 0.8807\n",
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.8116 - acc: 0.7304 - val_loss: 0.5712 - val_acc: 0.8043\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.5235 - acc: 0.8149 - val_loss: 0.4775 - val_acc: 0.8305\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.4651 - acc: 0.8348 - val_loss: 0.4461 - val_acc: 0.8427\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.4321 - acc: 0.8456 - val_loss: 0.4241 - val_acc: 0.8477\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.4090 - acc: 0.8534 - val_loss: 0.4158 - val_acc: 0.8487\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 1s 18us/step - loss: 0.3915 - acc: 0.8610 - val_loss: 0.3916 - val_acc: 0.8587\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.3773 - acc: 0.8652 - val_loss: 0.4132 - val_acc: 0.8462\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.3671 - acc: 0.8678 - val_loss: 0.3863 - val_acc: 0.8610\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 1s 15us/step - loss: 0.3575 - acc: 0.8729 - val_loss: 0.3800 - val_acc: 0.8625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.3488 - acc: 0.8737 - val_loss: 0.3666 - val_acc: 0.8679\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3402 - acc: 0.8777 - val_loss: 0.3712 - val_acc: 0.8651\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3324 - acc: 0.8812 - val_loss: 0.3510 - val_acc: 0.8749\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.3275 - acc: 0.8822 - val_loss: 0.3642 - val_acc: 0.8692\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3208 - acc: 0.8836 - val_loss: 0.3495 - val_acc: 0.8733\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3147 - acc: 0.8872 - val_loss: 0.3390 - val_acc: 0.8777\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 1s 15us/step - loss: 0.3100 - acc: 0.8886 - val_loss: 0.3402 - val_acc: 0.8781\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8906 - val_loss: 0.3605 - val_acc: 0.8687\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 1s 13us/step - loss: 0.3009 - acc: 0.8910 - val_loss: 0.3344 - val_acc: 0.8793\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2957 - acc: 0.8924 - val_loss: 0.3452 - val_acc: 0.8758\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2906 - acc: 0.8959 - val_loss: 0.3417 - val_acc: 0.8730\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.2862 - acc: 0.8954 - val_loss: 0.3292 - val_acc: 0.8795\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 1s 15us/step - loss: 0.2837 - acc: 0.8978 - val_loss: 0.3536 - val_acc: 0.8673\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 0s 10us/step - loss: 0.2804 - acc: 0.8988 - val_loss: 0.3231 - val_acc: 0.8833\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2766 - acc: 0.8998 - val_loss: 0.3287 - val_acc: 0.8781\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 1s 16us/step - loss: 0.2728 - acc: 0.9011 - val_loss: 0.3262 - val_acc: 0.8830\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.2680 - acc: 0.9014 - val_loss: 0.3295 - val_acc: 0.8799\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2659 - acc: 0.9024 - val_loss: 0.3385 - val_acc: 0.8763\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2631 - acc: 0.9048 - val_loss: 0.3241 - val_acc: 0.8829\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2603 - acc: 0.9055 - val_loss: 0.3288 - val_acc: 0.8799\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 1s 18us/step - loss: 0.2563 - acc: 0.9069 - val_loss: 0.3344 - val_acc: 0.8772\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.2552 - acc: 0.9075 - val_loss: 0.3200 - val_acc: 0.8829\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.2503 - acc: 0.9090 - val_loss: 0.3214 - val_acc: 0.8822\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 1s 18us/step - loss: 0.2475 - acc: 0.9106 - val_loss: 0.3205 - val_acc: 0.8836\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2454 - acc: 0.9115 - val_loss: 0.3156 - val_acc: 0.8860\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2434 - acc: 0.9124 - val_loss: 0.3226 - val_acc: 0.8829\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 1s 13us/step - loss: 0.2395 - acc: 0.9134 - val_loss: 0.3168 - val_acc: 0.8849\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2373 - acc: 0.9141 - val_loss: 0.3195 - val_acc: 0.8841\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2349 - acc: 0.9138 - val_loss: 0.3087 - val_acc: 0.8887\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2339 - acc: 0.9152 - val_loss: 0.3210 - val_acc: 0.8820\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 1s 13us/step - loss: 0.2291 - acc: 0.9165 - val_loss: 0.3190 - val_acc: 0.8834\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 1s 11us/step - loss: 0.2286 - acc: 0.9170 - val_loss: 0.3145 - val_acc: 0.8837\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2256 - acc: 0.9184 - val_loss: 0.3448 - val_acc: 0.8775\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2240 - acc: 0.9194 - val_loss: 0.3303 - val_acc: 0.8813\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2218 - acc: 0.9188 - val_loss: 0.3121 - val_acc: 0.8885\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 1s 19us/step - loss: 0.2187 - acc: 0.9206 - val_loss: 0.3140 - val_acc: 0.8885\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 1s 16us/step - loss: 0.2169 - acc: 0.9227 - val_loss: 0.3248 - val_acc: 0.8814\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 0s 11us/step - loss: 0.2151 - acc: 0.9216 - val_loss: 0.3216 - val_acc: 0.8851\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 1s 12us/step - loss: 0.2127 - acc: 0.9234 - val_loss: 0.3253 - val_acc: 0.8809\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 1s 17us/step - loss: 0.2124 - acc: 0.9229 - val_loss: 0.3093 - val_acc: 0.8899\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2088 - acc: 0.9244 - val_loss: 0.3212 - val_acc: 0.8855\n"
     ]
    }
   ],
   "source": [
    "history_list=[]\n",
    "for activation in ['relu','tanh']:\n",
    "    model = create_model([64]*1,activation,'softmax',0)\n",
    "    all_history = train_model(model,'categorical_crossentropy',0.001,1)    \n",
    "    history_list.append(all_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "- Train the data over 2 layers network with 64 unit\n",
    "- Consider, learning rate of 0.0001, 0.001, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "for lr in [0.0001,0.001,0.01]:\n",
    "    model = create_model([64]*1,'relu','softmax',0)\n",
    "    all_history = train_model(model,'categorical_crossentropy',lr,1)    \n",
    "    history_list.append(all_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- Use SGD with momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "model = create_model([64]*1,'relu','softmax',0)\n",
    "all_history = train_model(model,'categorical_crossentropy',0.001,0)    \n",
    "history_list.append(all_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Train Configuration\n",
    "- 2 layer with 64 unit\n",
    "- Epoch = 50 and Batch size = 512\n",
    "- Activation Function = *relu* and *softmax*\n",
    "- Optimizer *rmsprop* with learning rate 0.001\n",
    "- metrics = *accuracy* and loss function = *categorical_crossentropy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "model = create_model([64]*1,'relu','softmax',0)\n",
    "all_history = train_model(model,'categorical_crossentropy',0.1,1)    \n",
    "history_list.append(all_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training result\n",
    "check_train_result(history_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model([64]*1,'relu','softmax',0)\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 0.001),\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=1,batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To verify the prediction using `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation\n",
    "To improve the performance of the model it is important to tune its hyperparameters. Validation data helps to find out a good configuration by measuring the performance of model. But every time when we try to tune a hyperparameter of the model based on the model’s performance on the validation set, some information about the validation data leaks into the model. This leads us to build a model that performs artificially well on the validation data, but not on the new data. To find out a random set of validation data I used K-fold cross validation process in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "# https://www.tensorflow.org/tutorials/keras/basic_regression\n",
    "import keras as keras\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "    if epoch  == num_epochs - 1: print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model with k-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "(45000, 784)\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................................................................................... \n",
      "processing fold # 1\n",
      "(45000, 784)\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................................................................................... \n",
      "processing fold # 2\n",
      "(45000, 784)\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................................................................................... \n",
      "processing fold # 3\n",
      "(45000, 784)\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................................................................................... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_epochs = 600\n",
    "k = 4\n",
    "num_val_samples = len(x_train) // k\n",
    "all_loss_history = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    x_val = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    y_val = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    x_partial_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    y_partial_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    print(x_partial_train.shape)\n",
    "    \n",
    "   \n",
    "    model = create_model([64]*1,'relu','softmax',0)\n",
    "    \n",
    "       # model.compile(optimizer = optimizers.RMSprop(lr = 0.001),\n",
    "       # optimizer = optimizers.SGD(lr=0.1, decay=0.002, momentum=0.8, nesterov=False\n",
    "       # optimizers.SGD(lr=0.01, decay=0.0, momentum=0.7, nesterov=False)\n",
    "    model.compile(optimizer = optimizers.RMSprop(lr = 0.001),\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "    history = model.fit(x_partial_train, y_partial_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=num_epochs, \n",
    "                        batch_size=512, \n",
    "                        verbose=0, \n",
    "                        callbacks=[PrintDot()])\n",
    "    loss_history = history.history['val_loss']\n",
    "    all_loss_history.append(loss_history)\n",
    "    #history_dict = history.history\n",
    "    #print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find the average of the per-epoch val_acc scores for all folds and graph them:\n",
    "average_loss_history = [np.mean([x[i] for x in all_loss_history])for i in range(num_epochs)]\n",
    "plt.plot(range(1, len(average_loss_history) + 1), average_loss_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(average_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the plot easily I have removed the first 10 points (because they are on a different scale form the rest of the plot) and replace each point by an exponential moving average (to smooth the curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor = 0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points: # an empty list is 'False'\n",
    "            previous = smoothed_points[-1] # the last appended point\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_loss_history = smooth_curve(loss_history[10:])\n",
    "plt.plot(range(1, len(smooth_loss_history) + 1), smooth_loss_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test the model with K-fold**\n",
    "The vaidation accuracy stops improving after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(smooth_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 50us/step\n"
     ]
    }
   ],
   "source": [
    "model = create_model([64]*1,'relu','softmax',0)\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 0.001),\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=np.argmin(smooth_loss_history),batch_size=512,verbose = 0)\n",
    "test_loss_score, test_val_score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "- It is an important process to mitigate overfitting by introducing additional term into the cost function. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train data with regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "model = create_model([64]*1,'relu','softmax',1)\n",
    "all_history = train_model(model,'categorical_crossentropy',0.001,1)    \n",
    "history_list.append(all_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training result\n",
    "check_train_result(history_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test data with regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decay_v = 0.1/100 #epoc\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=decay_v, momentum=0.8, nesterov=False)\n",
    "#model.compile(optimizer = sgd,\n",
    "\n",
    "model = create_model([64]*1,'relu','softmax',1)\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 0.001),\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "   \n",
    "model.fit(x_train,y_train,epochs=38,batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model with drop out with regularization**\n",
    "- Dropout is a process to set zero randomly on a layer.\n",
    "- The dropout rate is usually set between 0.2 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list=[]\n",
    "model = create_model([64]*1,'relu','softmax',2)\n",
    "all_history = train_model(model,'categorical_crossentropy',0.001,1)    \n",
    "history_list.append(all_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training result\n",
    "check_train_result(history_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model with drop out with regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decay_v = 0.1/100 #epoc\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=decay_v, momentum=0.8, nesterov=False)\n",
    "#model.compile(optimizer = sgd,\n",
    "\n",
    "model = create_model([64]*1,'relu','softmax',2)\n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 0.001),\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=37,batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In this section all the findings created by the proposed model has been accumulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph(history_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Network Layer and Size**: \n",
    "- This model will classify 10 different categories as output. So, to prevent any information dropout I have selected input layer with more than 10 units. I have started building model with small network of 16 units with 2 layers. Later, increase the units and layers which includes 32,64,128,512 units with a combination of 2,3 and 4 layers. The graphs below presents both accuracy and loss of training and validation of these different types of networks. \n",
    "\n",
    "<img src=\"files/16_234.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig1: Graph of accuracy and loss during training (network of 16 units with 2,3,4 layers**\n",
    "<img src=\"files/32_234.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig2: Graph of accuracy and loss during training (network of 32 units with 2,3,4 layers**\n",
    "<img src=\"files/64_234.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig3: Graph of accuracy and loss during training (network of 64 units with 2,3,4 layers**\n",
    "<img src=\"files/128_234.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig4: Graph of accuracy and loss during training (network of 128 units with 2,3,4 layers**\n",
    "<img src=\"files/512_234.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig5: Graph of accuracy and loss during training (network of 512 units with 2,3,4 layers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to Fig1, we can see that small networks starts overfitting slowly and shows a lower validation loss which indicates a better model. But it's hypothesis space is small that can minimize the learning capability. So, I increased the network capacity. For bigger network,(Fig4 and Fig5) it started overfitting very quickly and validation loss is also noiser. To select an appropriate network I have considered these issues: lower difference between validation and training loss, percentage of validation accuracy, epochs of minimal validation loss which indicates starting point of overfitting. That's why I chose a 64 units network with 2 layers. The table below shows the result of maximum validation accuracy and position of minimum validation loss with different hidden units and layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameters | Layers | validation_acc      |  Position of minimum v_loss |\n",
    "|:---------------:|:------:|:-------------------:|:---------------------------:|\n",
    "|  Hidden Unit:16 |    2   |       0.866733      |              91             |\n",
    "| Batch Size: 512 |    3   |       0.874533      |              46             |\n",
    "|    Epoch: 100   |    4   |        0.8722       |              70             |\n",
    "|  Hidden Unit:32 |    2   | 0.8789333333333333  |              64             |\n",
    "| Batch Size: 512 |    3   |  0.8861333335876465 |              53             |\n",
    "|    Epoch: 100   |    4   |  0.8820000002861023 |              36             |\n",
    "|  Hidden Unit:64 |    2   |  0.8897333331743876 |              39             |\n",
    "| Batch Size: 512 |    3   | 0.8912000000953675  |              23             |\n",
    "|    Epoch: 100   |    4   |  0.8904666667620341 |              27             |\n",
    "| Hidden Unit:128 |    2   | 0.8938000000953674  |              24             |\n",
    "| Batch Size: 512 |    3   |  0.8952000001907349 |              17             |\n",
    "|    Epoch: 100   |    4   |  0.8920666668574015 |              16             |\n",
    "| Hidden Unit:512 |    2   |  0.9011333334287007 |              22             |\n",
    "| Batch Size: 512 |    3   | 0.8990666669527689  |              10             |\n",
    "|    Epoch: 100   |    4   |  0.9001999997456869 |              7              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Batch Size and Epoch:**\n",
    "Following table shows the combination of epochs and batch size and the result of it. After train the model for multiple time with these combinations I have selected my epoch no: 50 and batch size 512.\n",
    "\n",
    "| Epoch | Batch | Validation Accuracy | Test Accuracy |\n",
    "|-------|-------|---------------------|---------------|\n",
    "| 20    | 150   | 0.885733            | 0.8723        |\n",
    "| 50    | 300   | 0.8898              | 0.8825        |\n",
    "| 50    | 512   | 0.887266666762034   | 0.8875        |\n",
    "| 100   | 512   | 0.8918              | 0.8645        |\n",
    "\n",
    "\n",
    "**3. Learning Rate:**\n",
    "Fig6 shows the accuracy and loss during training with different learning rate. I have used three values: 0.0001, 0.001, 0.01 for this purpose. From the graph we can see that with higer learning rate(0.01) validation loss converges quickly and changes irregularly while lower (0.0001) one takes time to minimize validation. So, I choose 0.001 as learning rate for this model which also shows promising accuracy (show the following table).\n",
    "\n",
    "<img src=\"files/lr.png\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "                          **Fig6: Graph of accuracy and loss during training with different learning rate**\n",
    "\n",
    "|  Learning   Rate |  Validation Accuracy | Position of minimum val_loss|\n",
    "|:----------------:|:--------------------:|:----------------:|\n",
    "|      0.0001      |  0.8710000026226044  | 49             |\n",
    "|       0.001      |  0.8889999973773957  | 38             |          \n",
    "|       0.01       |  0.8764666676521301  | 11             |          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Activation function in middle hidden layer:** Because of the variation in testing accuracy showing in the following table I have chosen *relu* function.\n",
    "\n",
    "| Activation Function |  Val_acc         | Test_acc |\n",
    "|:-------------------:|:----------------:|---------:|\n",
    "|         relu        | 0.887266666762034| 88%      |\n",
    "|         tanh        | 0.888866666507721| 87%      |\n",
    "\n",
    "**5. Optimizer:** I have experimented with different optimizer (SGD and RMSprop) and change the learning rate. In both cases, RMSprop performs better.\n",
    "\n",
    "| Optimizer | Learning Rate | Validation Accuracy | Test Accuracy |\n",
    "|-----------|---------------|---------------------|---------------|\n",
    "| RMSprop   | 0.001         | 0.8886              | 87%           |\n",
    "| RMSprop   | 0.1           | 0.099467            | 99%           |\n",
    "| SGD       | 0.001         | 0.824667            | 82%           |\n",
    "| SGD       | 0.1           | 0.8842              | 88%           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Using K-fold:** Using k-fold with different epochs shows random changes in test accuracy.\n",
    "\n",
    "| Epoch | K-Fold | Test Accuracy |\n",
    "|:-----:|:------:|:-------------:|\n",
    "| 100   | 4      | 87%           |\n",
    "| 250   | 4      | 86%           |\n",
    "| 600   | 4      | 87%           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Adding Reularization and Dropout:** As we can see from the following table, that after incorporating regularization with the model test accuracy has been improved. However, adding dropout didn't change the test accuracy.\n",
    "\n",
    "| Parameter                   | Training Accuracy  | Test Accuracy |\n",
    "|-----------------------------|--------------------|---------------|\n",
    "| No Regularization           | 0.8886             | 87%           |\n",
    "| With Regularization         | 0.8895333330790202 | 88%           |\n",
    "| Regularization with Dropout | 0.8774             | 87%           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we can see that this approach reaches at the accuracy of ~88% after tuning different parameters.\n",
    "- To improve the model, more layer can be added to it. I think more no of epochs can help with this issue.\n",
    "- It is a simple network with two layers that minimizes the chance of being overfitted. I think for this reason dropout didn't work well on it.\n",
    "- Because of being small, it's hypothesis space is also small. It can hinder the model to get better accuracy.\n",
    "- As this example is a multiclass single-label classification problem, I have used softmax activation on last layer of the network. This function outputs a probability distribution over the N output classes. That means it will produce a probability of each class, whose sum is bound to be one.\n",
    "- Reason behind choosing categorical crossentropy as loss function is that it minimizes the distance between the probability distributions output by the network and the true distribution of the actual results.\n",
    "- K-fold cross validation helps the model to be generalized rather overfitted as it shuffles dataset and create random dataset to validate the model.\n",
    "- Adding regularization on model improves the result as it helps to spread out the weight values instead of having only a few weights with large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
