{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from skimage.transform import resize \n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "from keras import backend as k\n",
    "\n",
    "k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    def process_data(files,folder):\n",
    "        from skimage import data\n",
    "        label_dict={'1':'0','2':'1','3':'2','4':'3','5':'4','6':'5','7':'6','8':'7','9':'8','10':'9','11':'10'}\n",
    "        x=[]\n",
    "        y=[]\n",
    "        for file in files:\n",
    "            limg= data.imread(\"Dataset/\"+folder+'/'+file)\n",
    "            key = file.split('_')[-1].split('.')[0]\n",
    "            label_name = label_dict[key]\n",
    "        #img.resize(200,200)\n",
    "            y.append(label_name)\n",
    "            x.append(limg)\n",
    "        return x,y    \n",
    "    path, dirs, files = next(os.walk(\"Dataset/train\"))\n",
    "    path, dirs2, files2 = next(os.walk(\"Dataset/test\"))\n",
    "    path, dirs3, files3 = next(os.walk(\"Dataset/validation\"))\n",
    "    \n",
    "    rx_train, ry_train = process_data(files,'train')\n",
    "    rx_test, ry_test = process_data(files2,'test')\n",
    "    rx_val, ry_val = process_data(files3,'validation')\n",
    "\n",
    "\n",
    "    x_train=np.array(rx_train).reshape(len(rx_train),34,34,1)\n",
    "    x_test=np.array(rx_test).reshape(len(rx_test),34,34,1)\n",
    "    x_val=np.array(rx_val).reshape(len(rx_val),34,34,1)\n",
    "\n",
    "    X_train = np.array(x_train).astype('float32') / 255\n",
    "    X_test = np.array(x_test).astype('float32') / 255\n",
    "    X_val = np.array(x_val).astype('float32') / 255\n",
    "    Y_train = to_categorical(ry_train)\n",
    "    Y_test = to_categorical(ry_test)\n",
    "    Y_val = to_categorical(ry_val)\n",
    "    return X_train, Y_train, X_val, Y_val,X_test,Y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_val, Y_val):\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model_choice = {{choice(['one', 'two'])}}\n",
    "    if model_choice == 'one':\n",
    "        model.add(layers.Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(34,34,1)))\n",
    "        model.add(layers.Conv2D(16, kernel_size=3, activation='relu',padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "        model.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "    elif model_choice == 'two':\n",
    "        model.add(layers.Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(34,34,1)))\n",
    "        model.add(layers.Conv2D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "        model.add(layers.Conv2D(64, kernel_size=3, activation='relu'))\n",
    "        model.add(layers.Conv2D(64, kernel_size=3, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense({{choice([64, 128, 256])}}, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "    choiceval = {{choice(['one', 'two'])}}\n",
    "    if choiceval == 'two':\n",
    "        model.add(layers.Dense({{choice([64, 128, 256])}}, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(layers.Dense(11, activation='softmax'))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=adam)\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=128,\n",
    "              nb_epoch=20,\n",
    "              verbose=2,\n",
    "              validation_data=(X_val, Y_val))\n",
    "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Val accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os, shutil\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from skimage.transform import resize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import models\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as k\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from skimage import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'model_choice': hp.choice('model_choice', ['one', 'two']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [64, 128, 256]),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'model_choice_1': hp.choice('model_choice_1', ['one', 'two']),\n",
      "        'Dense_1': hp.choice('Dense_1', [64, 128, 256]),\n",
      "        'Dropout_5': hp.uniform('Dropout_5', 0, 1),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: def process_data(files,folder):\n",
      "   3:     from skimage import data\n",
      "   4:     label_dict={'1':'0','2':'1','3':'2','4':'3','5':'4','6':'5','7':'6','8':'7','9':'8','10':'9','11':'10'}\n",
      "   5:     x=[]\n",
      "   6:     y=[]\n",
      "   7:     for file in files:\n",
      "   8:         limg= data.imread(\"Dataset/\"+folder+'/'+file)\n",
      "   9:         key = file.split('_')[-1].split('.')[0]\n",
      "  10:         label_name = label_dict[key]\n",
      "  11:     #img.resize(200,200)\n",
      "  12:         y.append(label_name)\n",
      "  13:         x.append(limg)\n",
      "  14:     return x,y    \n",
      "  15: path, dirs, files = next(os.walk(\"Dataset/train\"))\n",
      "  16: path, dirs2, files2 = next(os.walk(\"Dataset/test\"))\n",
      "  17: path, dirs3, files3 = next(os.walk(\"Dataset/validation\"))\n",
      "  18: \n",
      "  19: rx_train, ry_train = process_data(files,'train')\n",
      "  20: rx_test, ry_test = process_data(files2,'test')\n",
      "  21: rx_val, ry_val = process_data(files3,'validation')\n",
      "  22: \n",
      "  23: \n",
      "  24: x_train=np.array(rx_train).reshape(len(rx_train),34,34,1)\n",
      "  25: x_test=np.array(rx_test).reshape(len(rx_test),34,34,1)\n",
      "  26: x_val=np.array(rx_val).reshape(len(rx_val),34,34,1)\n",
      "  27: \n",
      "  28: X_train = np.array(x_train).astype('float32') / 255\n",
      "  29: X_test = np.array(x_test).astype('float32') / 255\n",
      "  30: X_val = np.array(x_val).astype('float32') / 255\n",
      "  31: Y_train = to_categorical(ry_train)\n",
      "  32: Y_test = to_categorical(ry_test)\n",
      "  33: Y_val = to_categorical(ry_val)\n",
      "  34: \n",
      "  35: \n",
      "  36: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     model = models.Sequential()\n",
      "   5:     model_choice = space['model_choice']\n",
      "   6:     if model_choice == 'one':\n",
      "   7:         model.add(layers.Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(34,34,1)))\n",
      "   8:         model.add(layers.Conv2D(16, kernel_size=3, activation='relu',padding='same'))\n",
      "   9:         model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
      "  10:         model.add(layers.Dropout(space['Dropout']))\n",
      "  11: \n",
      "  12:         model.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
      "  13:         model.add(layers.Conv2D(32, kernel_size=3, activation='relu'))\n",
      "  14:         model.add(layers.BatchNormalization())\n",
      "  15:         model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
      "  16:         model.add(layers.Dropout(space['Dropout_1']))\n",
      "  17:     elif model_choice == 'two':\n",
      "  18:         model.add(layers.Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(34,34,1)))\n",
      "  19:         model.add(layers.Conv2D(32, kernel_size=3, activation='relu',padding='same'))\n",
      "  20:         model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
      "  21:         model.add(layers.Dropout(space['Dropout_2']))\n",
      "  22: \n",
      "  23:         model.add(layers.Conv2D(64, kernel_size=3, activation='relu'))\n",
      "  24:         model.add(layers.Conv2D(64, kernel_size=3, activation='relu'))\n",
      "  25:         model.add(layers.BatchNormalization())\n",
      "  26:         model.add(layers.MaxPooling2D(pool_size=2,strides=2))\n",
      "  27:         model.add(layers.Dropout(space['Dropout_3']))\n",
      "  28:     \n",
      "  29:     model.add(layers.Flatten())\n",
      "  30:     model.add(layers.Dense(space['Dense'], activation='relu'))\n",
      "  31:     model.add(layers.BatchNormalization())\n",
      "  32:     model.add(layers.Dropout(space['Dropout_4']))\n",
      "  33:     choiceval = space['model_choice_1']\n",
      "  34:     if choiceval == 'two':\n",
      "  35:         model.add(layers.Dense(space['Dense_1'], activation='relu'))\n",
      "  36:         model.add(layers.BatchNormalization())\n",
      "  37:         model.add(layers.Dropout(space['Dropout_5']))\n",
      "  38:     \n",
      "  39:     model.add(layers.Dense(11, activation='softmax'))\n",
      "  40:     \n",
      "  41:     adam = keras.optimizers.Adam(lr=0.001)\n",
      "  42:     \n",
      "  43:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      "  44:                   optimizer=adam)\n",
      "  45: \n",
      "  46:     model.fit(X_train, Y_train,\n",
      "  47:               batch_size=128,\n",
      "  48:               nb_epoch=20,\n",
      "  49:               verbose=2,\n",
      "  50:               validation_data=(X_val, Y_val))\n",
      "  51:     score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
      "  52:     print('Val accuracy:', acc)\n",
      "  53:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  54: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ovid2\\Documents\\AI\\temp_model.py:158: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13200 samples, validate on 4294 samples\n",
      "Epoch 1/20\n",
      " - 61s - loss: 2.4966 - acc: 0.3656 - val_loss: 1.4063 - val_acc: 0.5070\n",
      "Epoch 2/20\n",
      " - 64s - loss: 1.5779 - acc: 0.4985 - val_loss: 1.2542 - val_acc: 0.5638\n",
      "Epoch 3/20\n",
      " - 63s - loss: 1.3012 - acc: 0.5577 - val_loss: 1.1981 - val_acc: 0.5789\n",
      "Epoch 4/20\n",
      " - 62s - loss: 1.1445 - acc: 0.5952 - val_loss: 1.1437 - val_acc: 0.5971\n",
      "Epoch 5/20\n",
      " - 64s - loss: 1.0363 - acc: 0.6350 - val_loss: 1.0669 - val_acc: 0.6316\n",
      "Epoch 6/20\n",
      " - 64s - loss: 0.9744 - acc: 0.6543 - val_loss: 1.0064 - val_acc: 0.6660\n",
      "Epoch 7/20\n",
      " - 75s - loss: 0.8983 - acc: 0.6812 - val_loss: 1.0490 - val_acc: 0.6502\n",
      "Epoch 8/20\n",
      " - 113s - loss: 0.8301 - acc: 0.7016 - val_loss: 1.0082 - val_acc: 0.6581\n",
      "Epoch 9/20\n",
      " - 116s - loss: 0.7566 - acc: 0.7297 - val_loss: 0.9686 - val_acc: 0.6756\n",
      "Epoch 10/20\n",
      " - 116s - loss: 0.6967 - acc: 0.7561 - val_loss: 0.7692 - val_acc: 0.7401\n",
      "Epoch 11/20\n",
      " - 118s - loss: 0.6470 - acc: 0.7736 - val_loss: 0.7164 - val_acc: 0.7676\n",
      "Epoch 12/20\n",
      " - 117s - loss: 0.6142 - acc: 0.7873 - val_loss: 0.6588 - val_acc: 0.7878\n",
      "Epoch 13/20\n",
      " - 114s - loss: 0.5817 - acc: 0.7998 - val_loss: 0.6613 - val_acc: 0.7783\n",
      "Epoch 14/20\n",
      " - 110s - loss: 0.5553 - acc: 0.8111 - val_loss: 0.6404 - val_acc: 0.7799\n",
      "Epoch 15/20\n",
      " - 108s - loss: 0.5452 - acc: 0.8139 - val_loss: 0.6125 - val_acc: 0.8037\n",
      "Epoch 16/20\n",
      " - 117s - loss: 0.5104 - acc: 0.8265 - val_loss: 0.5421 - val_acc: 0.8237\n",
      "Epoch 17/20\n",
      " - 117s - loss: 0.4849 - acc: 0.8358 - val_loss: 0.4996 - val_acc: 0.8414\n",
      "Epoch 18/20\n",
      " - 116s - loss: 0.4740 - acc: 0.8433 - val_loss: 0.5158 - val_acc: 0.8414\n",
      "Epoch 19/20\n",
      " - 129s - loss: 0.4681 - acc: 0.8436 - val_loss: 0.5387 - val_acc: 0.8190\n",
      "Epoch 20/20\n",
      " - 124s - loss: 0.4514 - acc: 0.8467 - val_loss: 0.4677 - val_acc: 0.8540\n",
      "Val accuracy: 0.853982300857194\n",
      "Train on 13200 samples, validate on 4294 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train, X_val, Y_val,X_test,Y_test = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='OptimizingCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1024, activation='relu', input_shape=(34*34,)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.59))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.66))\n",
    "model.add(layers.Dense(11, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'Activation': 0, 'Activation_1': 0, 'Activation_2': 0, 'Dense': 3, 'Dense_1': 1, 'Dense_2': 0, 'Dropout': 0.2537673020472459, 'Dropout_1': 0.5899272287473123, 'Dropout_2': 0.6639991450877202, 'batch_size': 1, 'choiceval': 0, 'conditional': 1, 'lr': 0, 'lr_1': 1, 'lr_2': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,Y_train, X_val, Y_val = data()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(val_loss)+1, np.argmax(val_acc)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train, X_val, Y_val,X_test,Y_test = data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test,steps=7)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
